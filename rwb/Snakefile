# Felipe Giuste
# 5/17/2019
# Snakefile
# Randomise Whole Brain Nifti Analysis

import glob, os

#-------------------------------------------------------------------------------
# Constants
#-------------------------------------------------------------------------------
base_directory= "/mnt/gv0/"
nifti_directory= base_directory+"/FoxNiftis/"
temp_directory='/tmp_rwb/' # Output for randomise within Docker mapped onto Node filesystem
randomise_directory= base_directory+"/randomise" #"/HCP_Data/FG/fox_fullRes/randomise/" 
tstat_directory= base_directory+"/tstat"
nulltstat_directory= base_directory+"/nulltstat"
pval_directory= base_directory+"/pval"
model_directory = "/home/fgiuste/Github/Snakemake/rwb/" # base_directory+"/model/"
design_matrix = "group_comparison.mat"
contrast_matrix = "group_comparison.con"
npermutations = "5" # TODO:5000
ncontrasts=6

#-------------------------------------------------------------------------------
# Config
#-------------------------------------------------------------------------------
# SAMPLES: Folder name containing niftis for a row (128 Sources):
SAMPLES = [fname.split('/')[-1].split('.nii')[0] for fname in glob.glob('%s/*'%nifti_directory)]
# CONTRASTS: List of contrast integers as string:
CONTRASTS = [ str(i) for i in range(1,ncontrasts+1) ]


# Rowslices Randomise output:
RANDOMISE = expand(randomise_directory+"/{sample}_done", sample = SAMPLES)
# Finished Test T-Statistic Merging across chunks:
TSTAT = expand(tstat_directory+"/TestTval_{contrast}.zarr", contrast = CONTRASTS)
# TSTAT = tstat_directory+"_done"
# Finished Null T-Statistic Merging across chunks:
NULLTSTAT = expand(nulltstat_directory+"/NullTCounter_{contrast}.csv", contrast = CONTRASTS)
# NULLTSTAT = nulltstat_directory+"_done"
# Finished pvalue zarr files:
T2P = expand(pval_directory+"/corrPvals_{contrast}.zarr", contrast = CONTRASTS)
# T2P = pval_directory+"_done"


FINISHED_RANDOMISE = [fname.split('/')[-1] for fname in glob.glob('%s/*'%randomise_directory)]
FINISHED_TEMP = [fname.split('/')[-1] for fname in glob.glob('%s/*'%temp_directory)]
DELETE_RANDOMISE = [randomise_directory+"/"+fname+"_deleted" for fname in FINISHED_RANDOMISE]
DELETE_TEMP = [temp_directory+fname+"_deleted" for fname in FINISHED_TEMP]


#-------------------------------------------------------------------------------
# Rules
#-------------------------------------------------------------------------------
localrules: all, T2Pvals

rule all:
    input: T2P
    shell:
        """
        echo "Finito";
        """


# Execute Randomise on each nifti within a RowSlice folder: Starts Container=neuroimaging_{wildcards.sample}
# sample: rowSlice
# nii: chunk
rule randomise:
    input: niftidir= nifti_directory+"/{sample}"
    output: randomise_directory+"/{sample}_done" # RANDOMISE
    shell:
        """
        # Docker Container:
        docker run -t -d --rm -v {temp_directory}:{temp_directory}:rw -v {input[niftidir]}:/data:rw -v {model_directory}:/code:rw --name neuroimaging_{wildcards.sample} fgiuste/neuroimaging;
        
        chunklist=`ls {input[niftidir]} | sed s/.nii.gz//`
        for nii in $chunklist
        do
            if [ -d "{temp_directory}/{wildcards.sample}/${{nii}}_done" ]
            then
                echo "Found: {temp_directory}/{wildcards.sample}/${{nii}}_done; SKIPPING"
                continue
            fi

            # Create Chunk directory:
            mkdir -m777 -p {temp_directory}/{wildcards.sample}/${{nii}};

            # Randomise on Chunk within Container:
            run_randomise="randomise -i /data/${{nii}}.nii.gz -o {temp_directory}/{wildcards.sample}/${{nii}}/${{nii}} -d /code/{design_matrix} -t /code/{contrast_matrix} -n {npermutations} -R -N -x --permout";
            docker exec neuroimaging_{wildcards.sample} /bin/bash -c ". /home/startup.sh && ${{run_randomise}}"

            # Permutation processing:
            docker exec neuroimaging_{wildcards.sample} python /code/processPermutations.py {temp_directory}/{wildcards.sample}/${{nii}} {ncontrasts}
            echo 

            mv "{temp_directory}/{wildcards.sample}/${{nii}}" "{temp_directory}/{wildcards.sample}/${{nii}}_done"
        done

        # Shut down Container:
        docker stop neuroimaging_{wildcards.sample}

        # Copy final output to randomise_directory/sample_done:
        echo {output}
        mv {temp_directory}/{wildcards.sample} {output}
        """


# Merges Test tstat files from {randomise_directory}: Starts Container=neuroimaging_mergeTstats
# sample: rowSlice
rule mergeTstats:
    input:  RANDOMISE[0],
    output: tstatout= TSTAT #tstat_directory+"/TestTval_{contrast}.zarr" # TSTAT
    shell:
        """
        # Create tstat_directory:
        mkdir -m777 -p {tstat_directory}

        # Docker Container:
        # data -> randout
        # output -> tstat_directory
        docker run -t -d --rm -v {randomise_directory}:/data:rw -v {tstat_directory}:/output:rw -v {model_directory}:/code:rw --name neuroimaging_mergeTstats fgiuste/neuroimaging;
        
        # Permutation processing: randout, tstatout, ncontrasts
        docker exec neuroimaging_mergeTstats python /code/mergeTstats.py /data/ /output/ {ncontrasts}

        # Shut down Container:
        docker stop neuroimaging_mergeTstats

        # Rename tstat_directory to signify end:
        #mv {tstat_directory} {output}
        """


# Merges Null-Distribution tstat files across chunks: Starts Container=mergeNullTstats
rule mergeNullTstats:
    input:  RANDOMISE[0]
    output: nulltstatout= NULLTSTAT # nulltstat_directory+"/NullTCounter_{contrast}.csv" # NULLTSTAT
    shell:
        """
        # Create nulltstat_directory:
        mkdir -m777 -p {nulltstat_directory}

        # Docker Container:
        # data -> randout
        # output -> nulltstat_directory
        docker run -t -d --rm -v {randomise_directory}:/data:rw -v {nulltstat_directory}:/output:rw -v {model_directory}:/code:rw --name neuroimaging_mergeNullTstats fgiuste/neuroimaging;
        
        # NullTmerge: randout, nulltstat_directory, ncontrasts
        docker exec neuroimaging_mergeNullTstats python /code/mergeNullTstats.py /data/ /output/ {ncontrasts}

        # Shut down Container:
        docker stop neuroimaging_mergeNullTstats

        # Rename nulltstat_directory to signify end:
        #mv {nulltstat_directory} {output}
        """


# 
# Converts Test T-values into corrected P-values using Null-Distribution T-values
# Runs locally utilizing Dask via Slurm
rule T2Pvals:
    input:  tstatout= tstat_directory+"/TestTval_{contrast}.zarr", #TSTAT,
            nulltstatout= nulltstat_directory+"/NullTCounter_{contrast}.csv" # NULLTSTAT
    output: pvalout= pval_directory+"/corrPvals_{contrast}.zarr" # T2P
    shell:
        """
        # Create pval_directory:
        mkdir -m777 -p {pval_directory}

        # T2Pvals: tstatout, pval_directory, ncontrasts
        python {model_directory}/T2Pvals.py {tstat_directory} {nulltstat_directory} {pval_directory} {ncontrasts}

        # Rename pval_directory to signify end:
        #mv {pval_directory} {output}
        """





# DELETE RANDOMISE OUTPUT:
rule deleterandomise:
    input: DELETE_RANDOMISE
    shell:
        """
        rm {randomise_directory}/*_deleted
        echo "*Fin*";
        """

# DELETE RANDOMISE OUTPUT:
rule deletedeletedelete:
    input:  randout= randomise_directory+"/{sample}",
            tmpout= temp_directory+"/{sample}"
    output: randomise_directory+"/{sample}_deleted"
    shell:
        """
        rm -r {input[randout]};
        rm -r {input[tmpout]};
        touch {output}
        """


# TODO: create slurm job config file to specify ntasks=1,c=1, manually override for larger jobs

# RUN:
# cp ~/Github/Snakemake/rwb/*.py 
# snakemake --jobs 500 --cluster "sbatch -p SMIs -J 'rwbSMIs' --ntasks=1 --cpus-per-task=1 --mem=500M"



# Example:
# docker run -t -d --rm -v /mnt/gv0/FoxNiftis/512-639:/data:rw -v /HCP_Data/FG/fox_fullRes/randomise//512-639:/output:rw -v /mnt/gv0/model/:/code:rw --name neuroimaging_512-639 fgiuste/neuroimaging bash; 
# docker exec neuroimaging_512-639 /bin/bash -c ". /home/startup.sh && randomise -i /data/512-639_100096-100223.nii.gz -o /output/512-639_100096-100223/512-639_100096-100223 -d /code/group_comparison.mat -t /code/group_comparison.con -n 5000 -R -N -x --permout"; 

#docker exec neuroimaging_{wildcards.sample} /bin/bash -c ". /home/startup.sh && $run_randomise"
