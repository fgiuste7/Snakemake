# Felipe Giuste
# 5/17/2019
# Snakefile
# Randomise Whole Brain Nifti Analysis

import glob, os

#-------------------------------------------------------------------------------
# Constants
#-------------------------------------------------------------------------------
nifti_directory= "/mnt/gv0/FoxNiftis/"
randomise_directory= "/HCP_Data/FG/fox_fullRes/randomise/" # Changed from "/mnt/gv0/randomise/"
model_directory = "/mnt/gv0/model/"
design_matrix = "group_comparison.mat"
contrast_matrix = "group_comparison.con"
npermutations = "5000"
#ncontrasts = np.genfromtxt(contrasts, skip_header=4, comments='/').shape[0]


#-------------------------------------------------------------------------------
# Config
#-------------------------------------------------------------------------------
# SAMPLES: Folder name containing niftis for a row (128 Sources):
SAMPLES = [fname.split('/')[-1].split('.nii')[0] for fname in glob.glob('%s/*'%nifti_directory)]
RANDOMISE = expand(randomise_directory+"{sample}_done", sample = SAMPLES)

FINISHEDRANDOMISE = [fname.split('/')[-1] for fname in glob.glob('%s/*'%randomise_directory)]
DELETERANDOMISE = expand(randomise_directory+"{sample}_deleted", sample = FINISHEDRANDOMISE)

#-------------------------------------------------------------------------------
# Rules
#-------------------------------------------------------------------------------
rule all:
    input: RANDOMISE[0]
    shell:
        """
        echo "Finito";
        """


# Execute Randomise on each nifti within a RowSlice folder: Starts Container=neuroimaging_{wildcards.sample}
# docker run -it --rm -v /mnt/gv0/FoxNiftis/1024-1151/:/data/:rw -v /HCP_Data/FG/fox_fullRes/randomise/1024-1151/:/output:rw -v /mnt/gv0/model/:/code:rw --name neuroimaging fgiuste/neuroimaging
# randomise -i /data/1024-1151_100096-100223.nii.gz -o /output/1024-1151_100096-100223/1024-1151_100096-100223 -d /code/group_comparison.mat -t /code/group_comparison.con -n 5000 -R -N -x --permout
rule randomise:
    input: niftidir= nifti_directory+"{sample}"
    output: randomise_directory+"{sample}_done"
    shell:
        """
        mkdir -m777 -p {randomise_directory}/{wildcards.sample}
        echo "docker run -t -d --rm -v {input[niftidir]}:/data:rw -v {randomise_directory}/{wildcards.sample}:/output:rw -v {model_directory}:/code:rw --name neuroimaging_{wildcards.sample} fgiuste/neuroimaging";
        docker run -t -d --rm -v {input[niftidir]}:/data:rw -v {randomise_directory}/{wildcards.sample}:/output:rw -v {model_directory}:/code:rw --name neuroimaging_{wildcards.sample} fgiuste/neuroimaging;
        lslist=`ls {input[niftidir]} | sed s/.nii.gz//`
        for nii in $lslist
        do
            mkdir -m777 -p {randomise_directory}/{wildcards.sample}/$nii;
            run_randomise="randomise -i /data/$nii.nii.gz -o /output/$nii/$nii -d /code/{design_matrix} -t /code/{contrast_matrix} -n {npermutations} -R -N -x --permout";
            echo docker exec neuroimaging_{wildcards.sample} /bin/bash -c ". /home/startup.sh && $run_randomise";
            docker exec neuroimaging_{wildcards.sample} /bin/bash -c ". /home/startup.sh && $run_randomise"
            echo
        done
        docker stop neuroimaging_{wildcards.sample}
        mv {randomise_directory}/{wildcards.sample} {output}
        """

# rule permutation:
#     input: randomise_directory+"{sample}_done"
#     output: permutation_directory+"{sample}_done"
#     run:
#         # Iterate through contrasts:
#         ncontrasts=3
#         for contrast in range(1, ncontrasts+1):
#             perms= glob.glob('%s/*_vox_tstat%s_perm*.nii.gz' % ({output}, contrast) )
#             perms.sort(key= lambda x: int(x.split('_perm')[-1].split('.nii.gz')[0]) )

#             counterF= Counter()
#             for i in perms:
#                 # load nifti data:
#                 tmp= getNII(i)
#                 # round to tenths:
#                 tmp= np.round(tmp, decimals=1)
#                 # save as counter:
#                 countertmp= Counter(tmp.flatten())
#                 # Add to final counter:
#                 counterF= counterF + countertmp
#                 # delete perm data
#                 os.remove(i)

#             # Save counterF to {output} directory as 'NullTCounter.csv';
#             with open('%s/NullTCounter_%s.csv' % ({output}, contrast),'w') as csvfile:
#                 writer=csv.writer(csvfile, delimiter='\t')
#                 for key, value in counterF.items():
#                     writer.writerow([key,value])


# DELETE RANDOMISE OUTPUT:
rule deleterandomise:
    input: DELETERANDOMISE
    shell:
        """
        rm {randomise_directory}/*_deleted
        echo "*Fin*";
        """

# DELETE RANDOMISE OUTPUT:
rule deletedeletedelete:
    input: rowslice= randomise_directory+"{sample}"
    output: randomise_directory+"{sample}_deleted"
    shell:
        """
        rm -r {input[rowslice]};
        touch {output}
        """

# For Testing:
rule testing:
    input: niftidir= nifti_directory+"{sample}"
    output: randomise_directory+"{sample}_test"
    shell:
        """
        echo 'docker run -t -d --rm --name neuroimaging_{wildcards.sample} fgiuste/neuroimaging'
        docker run -t -d --rm --name neuroimaging_{wildcards.sample} fgiuste/neuroimaging
        mkdir -p {randomise_directory}/{wildcards.sample};
        for nii in `ls {input[niftidir]}`
        do
            run_randomise="randomise -i {input[niftidir]}/$nii -o {randomise_directory}/{wildcards.sample}/$nii/$nii -d {design_matrix} -t {contrast_matrix} -n {npermutations} -R -N -x --permout";
            docker exec neuroimaging_{wildcards.sample} echo $run_randomise > {randomise_directory}/{wildcards.sample}/$nii;
        done
        docker stop neuroimaging_{wildcards.sample}
        mv {randomise_directory}/{wildcards.sample} {output}
        """


# TODO: create slurm job config file to specify ntasks=1,c=1, manually override for larger jobs
## Also: slurm.conf: CR_CPU -> CR_CPU_Memory

# sudo salt 'SM*' cmd.run 'docker stop $(docker ps -q --filter ancestor=fgiuste/neuroimaging )'


# snakemake --jobs 500 --cluster "sbatch --ntasks=1 --cpus-per-task=1 --mem=500M"








# Example:
# docker run -t -d --rm -v /mnt/gv0/FoxNiftis/512-639:/data:rw -v /HCP_Data/FG/fox_fullRes/randomise//512-639:/output:rw -v /mnt/gv0/model/:/code:rw --name neuroimaging_512-639 fgiuste/neuroimaging bash; 
# docker exec neuroimaging_512-639 /bin/bash -c ". /home/startup.sh && randomise -i /data/512-639_100096-100223.nii.gz -o /output/512-639_100096-100223/512-639_100096-100223 -d /code/group_comparison.mat -t /code/group_comparison.con -n 5000 -R -N -x --permout"; 

#docker exec neuroimaging_{wildcards.sample} /bin/bash -c ". /home/startup.sh && $run_randomise"


# docker exec neuroimaging_512-639 /bin/bash -c '. /home/startup.sh && randomise'
